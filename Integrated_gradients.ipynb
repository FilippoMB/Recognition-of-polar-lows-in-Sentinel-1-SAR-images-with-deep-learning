{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ab6a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 23:09:50.065563: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.optimizers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95535045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "if not os.path.exists('models/model_1536_F1_095.h5'):\n",
    "    print(\"Downloading model...\")\n",
    "    file = requests.get(\"https://drive.google.com/uc?export=download&confirm=9_s_&id=1ErwopTT8ZKx-L9BspS5n1ATP4Za2K6Nh\")\n",
    "    open('models/model_1536_F1_095.h5', 'wb').write(file.content)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6611a5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 23:09:50.604698: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-06-27 23:09:50.657840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 23:09:50.658330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:08:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.845GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2022-06-27 23:09:50.658342: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-06-27 23:09:50.660184: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-06-27 23:09:50.660204: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-06-27 23:09:50.661197: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2022-06-27 23:09:50.661319: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2022-06-27 23:09:50.661557: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-06-27 23:09:50.661945: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-06-27 23:09:50.662015: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-06-27 23:09:50.662060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 23:09:50.662546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 23:09:50.663006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2022-06-27 23:09:50.663943: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-27 23:09:50.664664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 23:09:50.665134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:08:00.0 name: NVIDIA GeForce RTX 3090 computeCapability: 8.6\n",
      "coreClock: 1.845GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\n",
      "2022-06-27 23:09:50.665169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 23:09:50.665651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 23:09:50.666101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2022-06-27 23:09:50.666119: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-06-27 23:09:50.934434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-06-27 23:09:50.934457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2022-06-27 23:09:50.934462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2022-06-27 23:09:50.934600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 23:09:50.935105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 23:09:50.935576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-27 23:09:50.936032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22299 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:08:00.0, compute capability: 8.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1536, 1536,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Augment_and_Crop_and_Rescale (S (None, 1024, 1024, 3 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 512, 512, 8)  224         Augment_and_Crop_and_Rescale[0][0\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 512, 512, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 512, 512, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 512, 512, 8)  144         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 512, 512, 8)  32          separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512, 512, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 512, 512, 8)  144         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512, 512, 8)  32          separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512, 512, 8)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 256, 256, 8)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 8)  72          activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256, 256, 8)  0           max_pooling2d[0][0]              \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 256, 256, 16) 216         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256, 256, 16) 64          separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 256, 256, 16) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 256, 256, 16) 416         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256, 256, 16) 64          separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 256, 256, 16) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 16) 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 16) 144         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128, 128, 16) 0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 128, 128, 32) 688         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128, 128, 32) 128         separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128, 128, 32) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 128, 128, 32) 1344        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 32) 128         separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 32) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 32)   0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 32)   544         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 32)   0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 64, 64, 64)   2400        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 64)   256         separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 64, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 64, 64, 64)   4736        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 64)   256         separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 64)   0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 64)   2112        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 64)   0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 32, 32, 128)  8896        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 32, 32, 128)  17664       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 128)  512         separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 128)  8320        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 128)  0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 16, 16, 256)  34176       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 256)  1024        separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 16, 16, 256)  68096       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 256)  1024        separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 8, 256)    33024       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 8, 8, 256)    0           max_pooling2d_5[0][0]            \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 8, 8, 512)    133888      add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 512)    2048        separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 512)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 8, 8, 512)    267264      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 512)    2048        separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 512)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 4, 4, 512)    131584      add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4, 4, 512)    0           max_pooling2d_6[0][0]            \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 4, 4, 1024)   529920      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 4, 4, 1024)   4096        separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1024)         0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1024)         0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            2050        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,260,322\n",
      "Trainable params: 1,254,194\n",
      "Non-trainable params: 6,128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('models/model_1536_F1_095.h5', custom_objects={'AdamW': tfa.optimizers.AdamW})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1b5a8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 23:09:51.774366: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-06-27 23:09:51.790931: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3692830000 Hz\n",
      "2022-06-27 23:09:52.008127: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-06-27 23:09:52.348677: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8100\n",
      "2022-06-27 23:09:52.736873: W tensorflow/stream_executor/gpu/asm_compiler.cc:191] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6\n",
      "2022-06-27 23:09:52.736894: W tensorflow/stream_executor/gpu/asm_compiler.cc:194] Used ptxas at ptxas\n",
      "2022-06-27 23:09:52.736976: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Unimplemented: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2022-06-27 23:09:52.759452: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-06-27 23:09:53.152428: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-06-27 23:09:53.243711: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_261751/2637750095.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted class: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# Load image to test\n",
    "path = \"data/test/pos/\"\n",
    "img_name = \"cffe42_20191012T084028_20191012T084212_mos_rgb.png\"\n",
    "image_size = (1536, 1536)\n",
    "img = keras.preprocessing.image.load_img(\n",
    "    path + img_name, \n",
    "    target_size=image_size\n",
    ")\n",
    "img_array = keras.preprocessing.image.img_to_array(np.expand_dims(img,0)[0])\n",
    "\n",
    "pred = model.predict(tf.expand_dims(img_array, 0))\n",
    "print(\"Predicted class: \", preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97280355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_images(baseline,\n",
    "                       image,\n",
    "                       alphas):\n",
    "  alphas_x = alphas[:, tf.newaxis, tf.newaxis, tf.newaxis]\n",
    "  baseline_x = tf.expand_dims(baseline, axis=0)\n",
    "  input_x = tf.expand_dims(image, axis=0)\n",
    "  delta = input_x - baseline_x\n",
    "  images = baseline_x +  alphas_x * delta\n",
    "  return images\n",
    "\n",
    "\n",
    "def compute_gradients(images, target_class_idx, model):\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(images)\n",
    "    #logits = model(images, training=False)\n",
    "    #probs = tf.nn.softmax(logits, axis=-1)[:, target_class_idx]\n",
    "    probs = model(images, training=False)[:, target_class_idx]\n",
    "  return tape.gradient(probs, images)\n",
    "\n",
    "\n",
    "def integral_approximation(gradients):\n",
    "  # riemann_trapezoidal\n",
    "  grads = (gradients[:-1] + gradients[1:]) / tf.constant(2.0)\n",
    "  integrated_gradients = tf.math.reduce_mean(grads, axis=0)\n",
    "  return integrated_gradients\n",
    "\n",
    "\n",
    "@tf.function # compile the function into a high performance callable TensorFlow graph\n",
    "def integrated_gradients(baseline,\n",
    "                         image,\n",
    "                         model,\n",
    "                         target_class_idx,\n",
    "                         m_steps=50,\n",
    "                         batch_size=8):\n",
    "  # 1. Generate alphas.\n",
    "  alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps+1)\n",
    "\n",
    "  # Initialize TensorArray outside loop to collect gradients.    \n",
    "  gradient_batches = tf.TensorArray(tf.float32, size=m_steps+1)\n",
    "\n",
    "  # Iterate alphas range and batch computation for speed, memory efficiency, and scaling to larger m_steps.\n",
    "  for alpha in tf.range(0, len(alphas), batch_size):\n",
    "    from_ = alpha\n",
    "    to = tf.minimum(from_ + batch_size, len(alphas))\n",
    "    alpha_batch = alphas[from_:to]\n",
    "\n",
    "    # 2. Generate interpolated inputs between baseline and input.\n",
    "    interpolated_path_input_batch = interpolate_images(baseline=baseline,\n",
    "                                                       image=image,\n",
    "                                                       alphas=alpha_batch)\n",
    "\n",
    "    # 3. Compute gradients between model outputs and interpolated inputs.\n",
    "    gradient_batch = compute_gradients(model=model,\n",
    "                                       images=interpolated_path_input_batch,\n",
    "                                       target_class_idx=target_class_idx)\n",
    "\n",
    "    # Write batch indices and gradients to extend TensorArray.\n",
    "    gradient_batches = gradient_batches.scatter(tf.range(from_, to), gradient_batch)    \n",
    "    \n",
    "  # Stack path gradients together row-wise into single tensor.\n",
    "  total_gradients = gradient_batches.stack()\n",
    "\n",
    "  # 4. Integral approximation through averaging gradients.\n",
    "  avg_gradients = integral_approximation(gradients=total_gradients)\n",
    "\n",
    "  # 5. Scale integrated gradients with respect to input.\n",
    "  integrated_gradients = (image - baseline) * avg_gradients\n",
    "\n",
    "  return integrated_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = tf.zeros(shape=image_size +(3,))\n",
    "\n",
    "igrads = integrated_gradients(baseline=baseline,\n",
    "                              image=img_array,\n",
    "                              model=model,\n",
    "                              target_class_idx=1,\n",
    "                              m_steps=200,\n",
    "                              batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2789e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradVisualizer:\n",
    "    \"\"\"Plot gradients of the outputs w.r.t an input image.\"\"\"\n",
    "\n",
    "    def __init__(self, positive_channel=None, negative_channel=None):\n",
    "        if positive_channel is None:\n",
    "            self.positive_channel = [0, 255, 0]\n",
    "        else:\n",
    "            self.positive_channel = positive_channel\n",
    "\n",
    "        if negative_channel is None:\n",
    "            self.negative_channel = [255, 0, 0]\n",
    "        else:\n",
    "            self.negative_channel = negative_channel\n",
    "\n",
    "    def apply_polarity(self, attributions, polarity):\n",
    "        if polarity == \"positive\":\n",
    "            return np.clip(attributions, 0, 1)\n",
    "        else:\n",
    "            return np.clip(attributions, -1, 0)\n",
    "\n",
    "    def apply_linear_transformation(\n",
    "        self,\n",
    "        attributions,\n",
    "        clip_above_percentile=99.9,\n",
    "        clip_below_percentile=70.0,\n",
    "        lower_end=0.2,\n",
    "    ):\n",
    "        # 1. Get the thresholds\n",
    "        m = self.get_thresholded_attributions(\n",
    "            attributions, percentage=100 - clip_above_percentile\n",
    "        )\n",
    "        e = self.get_thresholded_attributions(\n",
    "            attributions, percentage=100 - clip_below_percentile\n",
    "        )\n",
    "\n",
    "        # 2. Transform the attributions by a linear function f(x) = a*x + b such that\n",
    "        # f(m) = 1.0 and f(e) = lower_end\n",
    "        transformed_attributions = (1 - lower_end) * (np.abs(attributions) - e) / (\n",
    "            m - e\n",
    "        ) + lower_end\n",
    "\n",
    "        # 3. Make sure that the sign of transformed attributions is the same as original attributions\n",
    "        transformed_attributions *= np.sign(attributions)\n",
    "\n",
    "        # 4. Only keep values that are bigger than the lower_end\n",
    "        transformed_attributions *= transformed_attributions >= lower_end\n",
    "\n",
    "        # 5. Clip values and return\n",
    "        transformed_attributions = np.clip(transformed_attributions, 0.0, 1.0)\n",
    "        return transformed_attributions\n",
    "\n",
    "    def get_thresholded_attributions(self, attributions, percentage):\n",
    "        if percentage == 100.0:\n",
    "            return np.min(attributions)\n",
    "\n",
    "        # 1. Flatten the attributions\n",
    "        flatten_attr = attributions.flatten()\n",
    "\n",
    "        # 2. Get the sum of the attributions\n",
    "        total = np.sum(flatten_attr)\n",
    "\n",
    "        # 3. Sort the attributions from largest to smallest.\n",
    "        sorted_attributions = np.sort(np.abs(flatten_attr))[::-1]\n",
    "\n",
    "        # 4. Calculate the percentage of the total sum that each attribution\n",
    "        # and the values about it contribute.\n",
    "        cum_sum = 100.0 * np.cumsum(sorted_attributions) / total\n",
    "\n",
    "        # 5. Threshold the attributions by the percentage\n",
    "        indices_to_consider = np.where(cum_sum >= percentage)[0][0]\n",
    "\n",
    "        # 6. Select the desired attributions and return\n",
    "        attributions = sorted_attributions[indices_to_consider]\n",
    "        return attributions\n",
    "\n",
    "    def binarize(self, attributions, threshold=0.001):\n",
    "        return attributions > threshold\n",
    "\n",
    "    def morphological_cleanup_fn(self, attributions, structure=np.ones((4, 4))):\n",
    "        closed = ndimage.grey_closing(attributions, structure=structure)\n",
    "        opened = ndimage.grey_opening(closed, structure=structure)\n",
    "        return opened\n",
    "\n",
    "    def draw_outlines(\n",
    "        self, attributions, percentage=90, connected_component_structure=np.ones((3, 3))\n",
    "    ):\n",
    "        # 1. Binarize the attributions.\n",
    "        attributions = self.binarize(attributions)\n",
    "\n",
    "        # 2. Fill the gaps\n",
    "        attributions = ndimage.binary_fill_holes(attributions)\n",
    "\n",
    "        # 3. Compute connected components\n",
    "        connected_components, num_comp = ndimage.measurements.label(\n",
    "            attributions, structure=connected_component_structure\n",
    "        )\n",
    "\n",
    "        # 4. Sum up the attributions for each component\n",
    "        total = np.sum(attributions[connected_components > 0])\n",
    "        component_sums = []\n",
    "        for comp in range(1, num_comp + 1):\n",
    "            mask = connected_components == comp\n",
    "            component_sum = np.sum(attributions[mask])\n",
    "            component_sums.append((component_sum, mask))\n",
    "\n",
    "        # 5. Compute the percentage of top components to keep\n",
    "        sorted_sums_and_masks = sorted(component_sums, key=lambda x: x[0], reverse=True)\n",
    "        sorted_sums = list(zip(*sorted_sums_and_masks))[0]\n",
    "        cumulative_sorted_sums = np.cumsum(sorted_sums)\n",
    "        cutoff_threshold = percentage * total / 100\n",
    "        cutoff_idx = np.where(cumulative_sorted_sums >= cutoff_threshold)[0][0]\n",
    "        if cutoff_idx > 2:\n",
    "            cutoff_idx = 2\n",
    "\n",
    "        # 6. Set the values for the kept components\n",
    "        border_mask = np.zeros_like(attributions)\n",
    "        for i in range(cutoff_idx + 1):\n",
    "            border_mask[sorted_sums_and_masks[i][1]] = 1\n",
    "\n",
    "        # 7. Make the mask hollow and show only the border\n",
    "        eroded_mask = ndimage.binary_erosion(border_mask, iterations=1)\n",
    "        border_mask[eroded_mask] = 0\n",
    "\n",
    "        # 8. Return the outlined mask\n",
    "        return border_mask\n",
    "\n",
    "    def process_grads(\n",
    "        self,\n",
    "        image,\n",
    "        attributions,\n",
    "        polarity=\"positive\",\n",
    "        clip_above_percentile=99.9,\n",
    "        clip_below_percentile=0,\n",
    "        morphological_cleanup=False,\n",
    "        structure=np.ones((3, 3)),\n",
    "        outlines=False,\n",
    "        outlines_component_percentage=90,\n",
    "        overlay=True,\n",
    "    ):\n",
    "        if polarity not in [\"positive\", \"negative\"]:\n",
    "            raise ValueError(\n",
    "                f\"\"\" Allowed polarity values: 'positive' or 'negative'\n",
    "                                    but provided {polarity}\"\"\"\n",
    "            )\n",
    "        if clip_above_percentile < 0 or clip_above_percentile > 100:\n",
    "            raise ValueError(\"clip_above_percentile must be in [0, 100]\")\n",
    "\n",
    "        if clip_below_percentile < 0 or clip_below_percentile > 100:\n",
    "            raise ValueError(\"clip_below_percentile must be in [0, 100]\")\n",
    "\n",
    "        # 1. Apply polarity\n",
    "        if polarity == \"positive\":\n",
    "            attributions = self.apply_polarity(attributions, polarity=polarity)\n",
    "            channel = self.positive_channel\n",
    "        else:\n",
    "            attributions = self.apply_polarity(attributions, polarity=polarity)\n",
    "            attributions = np.abs(attributions)\n",
    "            channel = self.negative_channel\n",
    "\n",
    "        # 2. Take average over the channels\n",
    "        attributions = np.average(attributions, axis=2)\n",
    "\n",
    "        # 3. Apply linear transformation to the attributions\n",
    "        attributions = self.apply_linear_transformation(\n",
    "            attributions,\n",
    "            clip_above_percentile=clip_above_percentile,\n",
    "            clip_below_percentile=clip_below_percentile,\n",
    "            lower_end=0.0,\n",
    "        )\n",
    "\n",
    "        # 4. Cleanup\n",
    "        if morphological_cleanup:\n",
    "            attributions = self.morphological_cleanup_fn(\n",
    "                attributions, structure=structure\n",
    "            )\n",
    "        # 5. Draw the outlines\n",
    "        if outlines:\n",
    "            attributions = self.draw_outlines(\n",
    "                attributions, percentage=outlines_component_percentage\n",
    "            )\n",
    "\n",
    "        # 6. Expand the channel axis and convert to RGB\n",
    "        attributions = np.expand_dims(attributions, 2) * channel\n",
    "\n",
    "        # 7.Superimpose on the original image\n",
    "        if overlay:\n",
    "            attributions = np.clip((attributions + image * 0.5), 0, 255)\n",
    "        return attributions\n",
    "\n",
    "    def visualize(\n",
    "        self,\n",
    "        image,\n",
    "        integrated_gradients,\n",
    "        polarity=\"positive\",\n",
    "        clip_above_percentile=99.9,\n",
    "        clip_below_percentile=0,\n",
    "        morphological_cleanup=False,\n",
    "        structure=np.ones((3, 3)),\n",
    "        outlines=False,\n",
    "        outlines_component_percentage=90,\n",
    "        overlay=True,\n",
    "        figsize=(15, 8),\n",
    "    ):\n",
    "        # Make a copy of the original image\n",
    "        img1 = np.copy(image)\n",
    "\n",
    "        # Process the integrated gradients\n",
    "        igrads_attr = self.process_grads(\n",
    "            image=img1,\n",
    "            attributions=integrated_gradients,\n",
    "            polarity=polarity,\n",
    "            clip_above_percentile=clip_above_percentile,\n",
    "            clip_below_percentile=clip_below_percentile,\n",
    "            morphological_cleanup=morphological_cleanup,\n",
    "            structure=structure,\n",
    "            outlines=outlines,\n",
    "            outlines_component_percentage=outlines_component_percentage,\n",
    "            overlay=overlay,\n",
    "        )\n",
    "\n",
    "        _, ax = plt.subplots(1, 2, figsize=figsize)\n",
    "        ax[0].imshow(image.astype(np.uint8))\n",
    "        ax[0].axis(\"off\")\n",
    "        ax[1].imshow(igrads_attr.astype(np.uint8))\n",
    "        ax[1].axis(\"off\")      \n",
    "\n",
    "        ax[0].set_title(\"Input\")\n",
    "        ax[1].set_title(\"Integrated gradients\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b9502",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis = GradVisualizer()\n",
    "vis.visualize(\n",
    "    image=img_array,\n",
    "    integrated_gradients=igrads.numpy(),\n",
    "    clip_above_percentile=99,\n",
    "    clip_below_percentile=0,\n",
    "    figsize=(15, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59780e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
